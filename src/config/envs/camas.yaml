## --- Environment ---
env: camas
env_args:
  map_name: "supermarket-medium"
  # Number of agents to use in the environment
  #     0: maximum number of agents
  #     Will return an error if number specifed is more than the maximum for the given map.
  agent_count: 0
  episode_limit: 0
  
  # Possible observation types: 
  #     global: [agent position, other agent positions.. ]
  #     context: [agent position, context of edges..  ]
  observation_type: "context"
  congestion: True
  dummy_actions: True

  # Divide reward by this factor before outputting
  reward_scaling_factor: 200

  # Mean and variance for Phase Type Distributions is calculated as:
  #     i*mean+1, var*(i+1)
  # where i is the congestion band and mean and var are set below.
  # If a Charlie map is used, the congestion bands from congestion_aware_planning are loaded in
  ptd_mean: 2
  ptd_var: 0.1
  two_bands_only: True

  # Rewards
  reward_type: "time-cost"
  goal_reward: 400
  time_multiplier: -3

  debug: False
  seed: Null  # NOTE this would be useful to implement

# -- Runner & Epsilon schedule ---
t_max: 15010000
runner: "async"
gamma: 0.99
epsilon_start: 1.0
epsilon_anneal_time: 2000000

# --- RL Method Edits ---
train_twice: False  # train for two mini batches after each episode
grad_norm_clip: 10

# --- Logging parameters ---
test_greedy: True
test_nepisode: 32
test_interval: 50000 # all used to be 10000
log_interval: 50000
runner_log_interval: 50000
learner_log_interval: 50000
save_model: True
save_model_interval: 500000
save_mac_weights: True  # save_model must also be True, saves at freq of save_model_interval
#checkpoint_path: "results/models/curriculum_qmix__2022-03-31_23-19-07/" # Load a checkpoint from this path

# --- Prioritised Replay ---
prioritised_replay: True # Whether to use Prioritised Experience Replay
per_alpha: 1.0  # defines shape of distribution, must lie in range [0, 1]
per_epsilon: 1.0  # constant added to reward value to ensure all episodes have a non zero probability of being sampled.
per_beta: 0.4  # importance sampling exponent, controls how much prioritization to apply. Must lie in the range [0, 1].
per_beta_anneal: 0.7  # percantage of t_max at which per_beta will be annealed to equal 1.
per_reward_power: 8 # scale rewards by this factor -- just for determining probabilities

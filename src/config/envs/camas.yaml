## --- Environment ---
env: camas
env_args:
  map_name: "supermarket-medium"
  # Number of agents to use in the environment
  #     0: maximum number of agents
  #     Will return an error if number specifed is more than the maximum for the given map.
  agent_count: 0
  episode_limit: 0
  
  # Possible observation types: 
  #     global: [agent position, other agent positions.. ]
  #     context: [agent position, context of edges..  ]
  observation_type: "context"
  congestion: True
  dummy_actions: True

  # Divide reward by this factor before outputting
  reward_scaling_factor: 100

  # Mean and variance for Phase Type Distributions is calculated as:
  #     i*mean+1, var*(i+1)
  # where i is the congestion band and mean and var are set below.
  ptd_mean: 1.0
  ptd_var: 0.1

  # Rewards
  reward_type: "time-cost"
  goal_reward: 200
  time_multiplier: -4

  debug: False
  seed: Null  # NOTE this would be useful to implement

# -- Runner & Epsilon schedule ---
t_max: 10010000
runner: "async"
gamma: 0.99
epsilon_start: 1.0
epsilon_anneal_time: 2000000

#buffer_size: 10000
train_twice: False

# --- Logging parameters ---
test_greedy: True
test_nepisode: 32
test_interval: 20000 # all used to be 10000
log_interval: 20000
runner_log_interval: 20000
learner_log_interval: 20000
save_model: True
save_model_interval: 500000
#checkpoint_path: "results/models/qmix__2022-03-23_18-35-08/" # Load a checkpoint from this path

# --- Prioritised Replay ---
prioritised_replay: True # Whether to use Prioritised Experience Replay
per_alpha: 1.0  # defines shape of distribution, must lie in range [0, 1]
per_epsilon: 0.5  # constant added to reward value to ensure all episodes have a non zero probability of being sampled.
per_beta: 0.4  # importance sampling exponent, controls how much prioritization to apply. Must lie in the range [0, 1].
per_beta_anneal: 0.7  # percantage of t_max at which per_beta will be annealed to equal 1.
